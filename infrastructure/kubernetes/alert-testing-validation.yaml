# ACT Placemat Alert Testing and Validation
# Comprehensive testing framework for alerting system

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: alert-testing-config
  namespace: monitoring
data:
  test-scenarios.yaml: |
    # ========================================
    # ALERT TESTING SCENARIOS
    # ========================================
    
    test_scenarios:
      
      # Infrastructure Tests
      infrastructure_tests:
        
        node_failure_simulation:
          name: "Node Failure Simulation"
          description: "Test Kubernetes node down alerts"
          type: "infrastructure"
          severity: "critical"
          steps:
            - action: "drain_node"
              target: "test-node-1"
              duration: "5m"
            - action: "verify_alert"
              alert_name: "KubernetesNodeDown"
              timeout: "2m"
            - action: "verify_escalation"
              policy: "infrastructure_critical"
              timeout: "1m"
            - action: "restore_node"
              target: "test-node-1"
          expected_alerts:
            - "KubernetesNodeDown"
            - "KubernetesPodRescheduled"
          expected_notifications:
            - channel: "slack"
              target: "#critical-alerts"
            - channel: "email"
              target: "ops@actplacemat.org.au"
            - channel: "pagerduty"
              service: "platform-primary"
        
        pod_crash_simulation:
          name: "Pod Crash Loop Simulation"
          description: "Test pod crash looping alerts"
          type: "infrastructure"
          severity: "critical"
          steps:
            - action: "deploy_failing_pod"
              image: "busybox:latest"
              command: ["sh", "-c", "exit 1"]
              replicas: 1
            - action: "wait_for_crashes"
              count: 3
              timeout: "10m"
            - action: "verify_alert"
              alert_name: "KubernetesPodCrashLooping"
              timeout: "5m"
            - action: "cleanup_pod"
          expected_alerts:
            - "KubernetesPodCrashLooping"
          expected_notifications:
            - channel: "slack"
              target: "#critical-alerts"
      
      # Performance Tests
      performance_tests:
        
        high_response_time_simulation:
          name: "High Response Time Simulation"
          description: "Test response time alerting"
          type: "performance"
          severity: "warning"
          steps:
            - action: "deploy_slow_service"
              delay: "3s"
              endpoint: "/test-slow"
            - action: "generate_load"
              requests_per_second: 10
              duration: "15m"
            - action: "verify_alert"
              alert_name: "HighResponseTime"
              timeout: "12m"
            - action: "cleanup_service"
          expected_alerts:
            - "HighResponseTime"
          expected_notifications:
            - channel: "slack"
              target: "#dev-alerts"
        
        high_error_rate_simulation:
          name: "High Error Rate Simulation"
          description: "Test error rate alerting"
          type: "performance"
          severity: "critical"
          steps:
            - action: "deploy_failing_service"
              error_rate: "10%"
              endpoint: "/test-errors"
            - action: "generate_load"
              requests_per_second: 20
              duration: "10m"
            - action: "verify_alert"
              alert_name: "HighErrorRate"
              timeout: "6m"
            - action: "cleanup_service"
          expected_alerts:
            - "HighErrorRate"
          expected_notifications:
            - channel: "slack"
              target: "#critical-alerts"
      
      # Security Tests
      security_tests:
        
        suspicious_login_simulation:
          name: "Suspicious Login Activity"
          description: "Test security alert for suspicious logins"
          type: "security"
          severity: "warning"
          steps:
            - action: "simulate_failed_logins"
              rate: "15/minute"
              duration: "5m"
              source_ip: "192.168.1.100"
            - action: "verify_alert"
              alert_name: "SuspiciousLoginActivity"
              timeout: "3m"
            - action: "verify_escalation"
              policy: "security_critical"
              timeout: "1m"
          expected_alerts:
            - "SuspiciousLoginActivity"
          expected_notifications:
            - channel: "slack"
              target: "#security-alerts"
            - channel: "email"
              target: "security@actplacemat.org.au"
        
        data_export_volume_test:
          name: "Unusual Data Export Volume"
          description: "Test data export monitoring"
          type: "security"
          severity: "warning"
          steps:
            - action: "simulate_data_exports"
              count: 15
              duration: "1h"
              user_id: "test-user-123"
            - action: "verify_alert"
              alert_name: "UnusualDataExportVolume"
              timeout: "20m"
          expected_alerts:
            - "UnusualDataExportVolume"
          expected_notifications:
            - channel: "email"
              target: "privacy-officer@actplacemat.org.au"
      
      # Cultural Protocol Tests
      cultural_tests:
        
        cultural_protocol_violation_test:
          name: "Cultural Protocol Violation"
          description: "Test cultural protocol alerting"
          type: "cultural"
          severity: "critical"
          steps:
            - action: "simulate_protocol_violation"
              violation_type: "unauthorized_cultural_content_access"
              community_id: "test-community-001"
            - action: "verify_alert"
              alert_name: "CulturalProtocolViolation"
              timeout: "1m"
            - action: "verify_escalation"
              policy: "cultural_critical"
              timeout: "30s"
          expected_alerts:
            - "CulturalProtocolViolation"
          expected_notifications:
            - channel: "slack"
              target: "#cultural-protocols"
            - channel: "email"
              target: "cultural-advisors@actplacemat.org.au"
        
        elder_review_backlog_test:
          name: "Elder Review Backlog"
          description: "Test elder review monitoring"
          type: "cultural"
          severity: "warning"
          steps:
            - action: "create_pending_reviews"
              count: 8
              age: "2h"
            - action: "verify_alert"
              alert_name: "ElderReviewBacklog"
              timeout: "5m"
          expected_alerts:
            - "ElderReviewBacklog"
          expected_notifications:
            - channel: "email"
              target: "elder-council@actplacemat.org.au"
      
      # Community Health Tests
      community_tests:
        
        community_engagement_drop_test:
          name: "Community Engagement Drop"
          description: "Test community health monitoring"
          type: "community"
          severity: "warning"
          steps:
            - action: "simulate_low_engagement"
              community_id: "test-community-002"
              engagement_rate: "0.03"
              duration: "45m"
            - action: "verify_alert"
              alert_name: "CommunityEngagementDrop"
              timeout: "35m"
          expected_alerts:
            - "CommunityEngagementDrop"
          expected_notifications:
            - channel: "slack"
              target: "#community-alerts"
        
        high_churn_rate_test:
          name: "High Community Churn Rate"
          description: "Test churn rate monitoring"
          type: "community"
          severity: "warning"
          steps:
            - action: "simulate_member_departures"
              community_id: "test-community-003"
              departure_rate: "15%"
              duration: "2h"
            - action: "verify_alert"
              alert_name: "HighCommunityChurnRate"
              timeout: "1.5h"
          expected_alerts:
            - "HighCommunityChurnRate"
          expected_notifications:
            - channel: "email"
              target: "community@actplacemat.org.au"
      
      # Business KPI Tests
      business_kpi_tests:
        
        opportunity_applications_low_test:
          name: "Low Opportunity Applications"
          description: "Test opportunity application monitoring"
          type: "business"
          severity: "warning"
          steps:
            - action: "simulate_low_applications"
              application_count: 1
              duration: "8h"
            - action: "verify_alert"
              alert_name: "OpportunityApplicationsLow"
              timeout: "6.5h"
          expected_alerts:
            - "OpportunityApplicationsLow"
          expected_notifications:
            - channel: "email"
              target: "partnerships@actplacemat.org.au"
        
        platform_efficiency_test:
          name: "Platform Efficiency Low"
          description: "Test platform efficiency monitoring"
          type: "business"
          severity: "warning"
          steps:
            - action: "simulate_low_efficiency"
              efficiency_score: "1.5"
              duration: "48h"
            - action: "verify_alert"
              alert_name: "PlatformEfficiencyLow"
              timeout: "48.5h"
          expected_alerts:
            - "PlatformEfficiencyLow"
          expected_notifications:
            - channel: "email"
              target: "operations@actplacemat.org.au"
    
    # ========================================
    # ESCALATION TESTING SCENARIOS
    # ========================================
    
    escalation_test_scenarios:
      
      critical_infrastructure_escalation:
        name: "Critical Infrastructure Escalation Path"
        description: "Test full escalation for critical infrastructure failure"
        alert_type: "KubernetesNodeDown"
        escalation_policy: "infrastructure_critical"
        test_steps:
          - time: "0m"
            expected_contacts: ["platform-oncall", "ops-team"]
            expected_channels: ["email", "slack", "pagerduty"]
          - time: "5m"
            expected_contacts: ["platform-manager", "cto"]
            expected_channels: ["email", "phone", "slack"]
          - time: "15m"
            expected_contacts: ["executive-team", "ceo"]
            expected_channels: ["email", "phone"]
      
      cultural_protocol_escalation:
        name: "Cultural Protocol Violation Escalation"
        description: "Test escalation for cultural protocol violations"
        alert_type: "CulturalProtocolViolation"
        escalation_policy: "cultural_critical"
        test_steps:
          - time: "0m"
            expected_contacts: ["cultural-advisors", "community-managers"]
            expected_channels: ["email", "slack"]
          - time: "10m"
            expected_contacts: ["elder-council", "cultural-manager"]
            expected_channels: ["email", "phone"]
          - time: "30m"
            expected_contacts: ["ceo", "board-cultural-advisor"]
            expected_channels: ["email", "phone"]
      
      security_incident_escalation:
        name: "Security Incident Escalation Path"
        description: "Test escalation for security incidents"
        alert_type: "SuspiciousLoginActivity"
        escalation_policy: "security_critical"
        test_steps:
          - time: "0m"
            expected_contacts: ["security-oncall", "security-team"]
            expected_channels: ["email", "slack", "pagerduty"]
          - time: "3m"
            expected_contacts: ["ciso", "security-manager"]
            expected_channels: ["email", "phone", "slack"]
          - time: "10m"
            expected_contacts: ["legal-team", "cto", "ceo"]
            expected_channels: ["email", "phone"]
    
    # ========================================
    # NOTIFICATION TESTING
    # ========================================
    
    notification_tests:
      
      email_delivery_test:
        name: "Email Notification Delivery"
        description: "Test email notification delivery and formatting"
        channels: ["email"]
        test_recipients:
          - "test-ops@actplacemat.org.au"
          - "test-security@actplacemat.org.au"
          - "test-cultural@actplacemat.org.au"
        test_alerts:
          - alert_name: "TestAlert"
            severity: "warning"
            template: "email.default.html"
          - alert_name: "TestCriticalAlert"
            severity: "critical"
            template: "email.critical.html"
          - alert_name: "TestCulturalAlert"
            severity: "warning"
            template: "email.cultural.html"
        validation_criteria:
          - "Email delivered within 30 seconds"
          - "Template renders correctly"
          - "All placeholders replaced"
          - "Links are functional"
      
      slack_delivery_test:
        name: "Slack Notification Delivery"
        description: "Test Slack notification delivery and formatting"
        channels: ["slack"]
        test_channels:
          - "#test-alerts"
          - "#test-critical"
          - "#test-cultural"
        test_alerts:
          - alert_name: "TestSlackAlert"
            severity: "warning"
            template: "slack.default"
          - alert_name: "TestSlackCritical"
            severity: "critical"
            template: "slack.critical"
        validation_criteria:
          - "Message delivered within 10 seconds"
          - "Formatting preserved"
          - "Buttons functional"
          - "Channel routing correct"
      
      pagerduty_integration_test:
        name: "PagerDuty Integration Test"
        description: "Test PagerDuty integration and escalation"
        channels: ["pagerduty"]
        test_services:
          - "ACT-Platform-Primary"
          - "ACT-Security-Primary"
        test_alerts:
          - alert_name: "TestPagerDutyAlert"
            severity: "critical"
            service: "ACT-Platform-Primary"
        validation_criteria:
          - "Incident created in PagerDuty"
          - "Correct service assignment"
          - "Proper severity mapping"
          - "Escalation policy triggered"

---
# Alert Testing Framework Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: alert-testing-framework
  namespace: monitoring
  labels:
    app: alert-testing-framework
spec:
  replicas: 1
  selector:
    matchLabels:
      app: alert-testing-framework
  template:
    metadata:
      labels:
        app: alert-testing-framework
    spec:
      serviceAccountName: alert-tester
      containers:
      - name: alert-tester
        image: python:3.11-slim
        command: ["/bin/sh"]
        args:
          - -c
          - |
            pip install kubernetes requests pyyaml schedule prometheus-client
            python /app/alert_tester.py
        env:
        - name: KUBERNETES_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: PROMETHEUS_URL
          value: "http://prometheus-service:9090"
        - name: ALERTMANAGER_URL
          value: "http://alertmanager-service:9093"
        volumeMounts:
        - name: testing-config
          mountPath: /config
        - name: testing-script
          mountPath: /app
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "200m"
      volumes:
      - name: testing-config
        configMap:
          name: alert-testing-config
      - name: testing-script
        configMap:
          name: alert-testing-script

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: alert-testing-script
  namespace: monitoring
data:
  alert_tester.py: |
    #!/usr/bin/env python3
    """
    ACT Placemat Alert Testing Framework
    Automated testing of alerting system and escalation policies
    """
    
    import os
    import time
    import yaml
    import json
    import logging
    import requests
    import schedule
    from datetime import datetime, timedelta
    from kubernetes import client, config
    from typing import Dict, List, Any, Optional
    
    # Configure logging
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    
    class AlertTester:
        def __init__(self):
            # Load Kubernetes config
            try:
                config.load_incluster_config()
            except:
                config.load_kube_config()
            
            self.k8s_apps_v1 = client.AppsV1Api()
            self.k8s_core_v1 = client.CoreV1Api()
            
            self.prometheus_url = os.getenv('PROMETHEUS_URL', 'http://prometheus-service:9090')
            self.alertmanager_url = os.getenv('ALERTMANAGER_URL', 'http://alertmanager-service:9093')
            self.namespace = os.getenv('KUBERNETES_NAMESPACE', 'monitoring')
            
            self.test_config = self.load_test_config()
            self.test_results = []
            
        def load_test_config(self) -> Dict[str, Any]:
            """Load test configuration"""
            try:
                with open('/config/test-scenarios.yaml', 'r') as f:
                    return yaml.safe_load(f)
            except Exception as e:
                logger.error(f"Failed to load test config: {e}")
                return {}
        
        def send_test_alert(self, alert_name: str, severity: str, labels: Dict[str, str] = None) -> bool:
            """Send a test alert to Alertmanager"""
            try:
                if labels is None:
                    labels = {}
                
                alert_payload = {
                    "alerts": [{
                        "labels": {
                            "alertname": alert_name,
                            "severity": severity,
                            "test": "true",
                            "instance": "test-instance",
                            **labels
                        },
                        "annotations": {
                            "summary": f"Test alert: {alert_name}",
                            "description": f"This is a test alert for {alert_name}",
                            "test_timestamp": datetime.now().isoformat()
                        },
                        "startsAt": datetime.now().isoformat(),
                        "generatorURL": "http://alert-tester/test"
                    }]
                }
                
                response = requests.post(
                    f"{self.alertmanager_url}/api/v1/alerts",
                    json=alert_payload,
                    headers={"Content-Type": "application/json"}
                )
                
                if response.status_code == 200:
                    logger.info(f"Successfully sent test alert: {alert_name}")
                    return True
                else:
                    logger.error(f"Failed to send test alert: {response.status_code} - {response.text}")
                    return False
                    
            except Exception as e:
                logger.error(f"Error sending test alert: {e}")
                return False
        
        def wait_for_alert(self, alert_name: str, timeout: int = 300) -> bool:
            """Wait for an alert to appear in Alertmanager"""
            try:
                start_time = time.time()
                
                while time.time() - start_time < timeout:
                    response = requests.get(f"{self.alertmanager_url}/api/v1/alerts")
                    
                    if response.status_code == 200:
                        alerts = response.json().get('data', [])
                        
                        for alert in alerts:
                            if alert.get('labels', {}).get('alertname') == alert_name:
                                logger.info(f"Found alert: {alert_name}")
                                return True
                    
                    time.sleep(10)  # Check every 10 seconds
                
                logger.warning(f"Timeout waiting for alert: {alert_name}")
                return False
                
            except Exception as e:
                logger.error(f"Error waiting for alert: {e}")
                return False
        
        def create_failing_pod(self, name: str) -> bool:
            """Create a pod that will crash loop for testing"""
            try:
                pod_spec = client.V1Pod(
                    metadata=client.V1ObjectMeta(
                        name=name,
                        namespace=self.namespace,
                        labels={"test": "alert-testing", "type": "failing-pod"}
                    ),
                    spec=client.V1PodSpec(
                        containers=[
                            client.V1Container(
                                name="failing-container",
                                image="busybox:latest",
                                command=["sh", "-c", "echo 'Crashing for test'; exit 1"],
                                restart_policy="Always"
                            )
                        ],
                        restart_policy="Always"
                    )
                )
                
                self.k8s_core_v1.create_namespaced_pod(
                    namespace=self.namespace,
                    body=pod_spec
                )
                
                logger.info(f"Created failing pod: {name}")
                return True
                
            except Exception as e:
                logger.error(f"Failed to create failing pod: {e}")
                return False
        
        def cleanup_test_pod(self, name: str) -> bool:
            """Clean up test pod"""
            try:
                self.k8s_core_v1.delete_namespaced_pod(
                    name=name,
                    namespace=self.namespace
                )
                logger.info(f"Cleaned up test pod: {name}")
                return True
            except Exception as e:
                logger.error(f"Failed to cleanup pod: {e}")
                return False
        
        def simulate_high_response_time(self, duration: int = 600) -> bool:
            """Simulate high response time by creating slow service"""
            try:
                # Create a deployment with artificially slow responses
                deployment_spec = client.V1Deployment(
                    metadata=client.V1ObjectMeta(
                        name="slow-test-service",
                        namespace=self.namespace,
                        labels={"test": "alert-testing", "type": "slow-service"}
                    ),
                    spec=client.V1DeploymentSpec(
                        replicas=1,
                        selector=client.V1LabelSelector(
                            match_labels={"app": "slow-test-service"}
                        ),
                        template=client.V1PodTemplateSpec(
                            metadata=client.V1ObjectMeta(
                                labels={"app": "slow-test-service", "test": "alert-testing"}
                            ),
                            spec=client.V1PodSpec(
                                containers=[
                                    client.V1Container(
                                        name="slow-app",
                                        image="nginx:alpine",
                                        ports=[client.V1ContainerPort(container_port=80)],
                                        env=[
                                            client.V1EnvVar(name="RESPONSE_DELAY", value="3")  # 3 second delay
                                        ]
                                    )
                                ]
                            )
                        )
                    )
                )
                
                self.k8s_apps_v1.create_namespaced_deployment(
                    namespace=self.namespace,
                    body=deployment_spec
                )
                
                logger.info("Created slow test service")
                
                # Wait for the specified duration
                time.sleep(duration)
                
                # Cleanup
                self.k8s_apps_v1.delete_namespaced_deployment(
                    name="slow-test-service",
                    namespace=self.namespace
                )
                
                logger.info("Cleaned up slow test service")
                return True
                
            except Exception as e:
                logger.error(f"Failed to simulate high response time: {e}")
                return False
        
        def run_infrastructure_test(self, test_config: Dict[str, Any]) -> Dict[str, Any]:
            """Run infrastructure-related tests"""
            test_name = test_config.get('name', 'Unknown Test')
            logger.info(f"Running infrastructure test: {test_name}")
            
            result = {
                'test_name': test_name,
                'test_type': 'infrastructure',
                'start_time': datetime.now().isoformat(),
                'success': False,
                'details': {}
            }
            
            try:
                if 'pod_crash_simulation' in test_name.lower():
                    # Test pod crash loop detection
                    pod_name = f"test-crash-pod-{int(time.time())}"
                    
                    if self.create_failing_pod(pod_name):
                        result['details']['pod_created'] = True
                        
                        # Wait for crash loop alert
                        if self.wait_for_alert('KubernetesPodCrashLooping', timeout=600):
                            result['details']['alert_triggered'] = True
                            result['success'] = True
                        else:
                            result['details']['alert_triggered'] = False
                        
                        # Cleanup
                        self.cleanup_test_pod(pod_name)
                        result['details']['cleaned_up'] = True
                    
                elif 'node_failure_simulation' in test_name.lower():
                    # Simulate node failure by sending alert directly
                    if self.send_test_alert('KubernetesNodeDown', 'critical', {'instance': 'test-node-1'}):
                        result['details']['alert_sent'] = True
                        result['success'] = True
                    
            except Exception as e:
                result['details']['error'] = str(e)
                logger.error(f"Error in infrastructure test: {e}")
            
            result['end_time'] = datetime.now().isoformat()
            return result
        
        def run_performance_test(self, test_config: Dict[str, Any]) -> Dict[str, Any]:
            """Run performance-related tests"""
            test_name = test_config.get('name', 'Unknown Test')
            logger.info(f"Running performance test: {test_name}")
            
            result = {
                'test_name': test_name,
                'test_type': 'performance',
                'start_time': datetime.now().isoformat(),
                'success': False,
                'details': {}
            }
            
            try:
                if 'high_response_time' in test_name.lower():
                    # Send high response time alert directly
                    if self.send_test_alert('HighResponseTime', 'warning', {'endpoint': '/test'}):
                        result['details']['alert_sent'] = True
                        result['success'] = True
                
                elif 'high_error_rate' in test_name.lower():
                    # Send high error rate alert
                    if self.send_test_alert('HighErrorRate', 'critical', {'service': 'test-service'}):
                        result['details']['alert_sent'] = True
                        result['success'] = True
                
            except Exception as e:
                result['details']['error'] = str(e)
                logger.error(f"Error in performance test: {e}")
            
            result['end_time'] = datetime.now().isoformat()
            return result
        
        def run_security_test(self, test_config: Dict[str, Any]) -> Dict[str, Any]:
            """Run security-related tests"""
            test_name = test_config.get('name', 'Unknown Test')
            logger.info(f"Running security test: {test_name}")
            
            result = {
                'test_name': test_name,
                'test_type': 'security',
                'start_time': datetime.now().isoformat(),
                'success': False,
                'details': {}
            }
            
            try:
                if 'suspicious_login' in test_name.lower():
                    # Send suspicious login alert
                    if self.send_test_alert('SuspiciousLoginActivity', 'warning', {'source_ip': '192.168.1.100'}):
                        result['details']['alert_sent'] = True
                        result['success'] = True
                
                elif 'data_export' in test_name.lower():
                    # Send data export alert
                    if self.send_test_alert('UnusualDataExportVolume', 'warning', {'user_id': 'test-user'}):
                        result['details']['alert_sent'] = True
                        result['success'] = True
                
            except Exception as e:
                result['details']['error'] = str(e)
                logger.error(f"Error in security test: {e}")
            
            result['end_time'] = datetime.now().isoformat()
            return result
        
        def run_cultural_test(self, test_config: Dict[str, Any]) -> Dict[str, Any]:
            """Run cultural protocol tests"""
            test_name = test_config.get('name', 'Unknown Test')
            logger.info(f"Running cultural test: {test_name}")
            
            result = {
                'test_name': test_name,
                'test_type': 'cultural',
                'start_time': datetime.now().isoformat(),
                'success': False,
                'details': {}
            }
            
            try:
                if 'protocol_violation' in test_name.lower():
                    # Send cultural protocol violation alert
                    if self.send_test_alert('CulturalProtocolViolation', 'critical', 
                                          {'community_id': 'test-community', 'violation_type': 'test'}):
                        result['details']['alert_sent'] = True
                        result['success'] = True
                
                elif 'elder_review' in test_name.lower():
                    # Send elder review backlog alert
                    if self.send_test_alert('ElderReviewBacklog', 'warning', {'pending_count': '8'}):
                        result['details']['alert_sent'] = True
                        result['success'] = True
                
            except Exception as e:
                result['details']['error'] = str(e)
                logger.error(f"Error in cultural test: {e}")
            
            result['end_time'] = datetime.now().isoformat()
            return result
        
        def run_test_suite(self) -> Dict[str, Any]:
            """Run complete test suite"""
            logger.info("Starting alert testing suite")
            
            suite_results = {
                'start_time': datetime.now().isoformat(),
                'tests': [],
                'summary': {}
            }
            
            test_scenarios = self.test_config.get('test_scenarios', {})
            
            # Run infrastructure tests
            for test_name, test_config in test_scenarios.get('infrastructure_tests', {}).items():
                result = self.run_infrastructure_test(test_config)
                suite_results['tests'].append(result)
            
            # Run performance tests
            for test_name, test_config in test_scenarios.get('performance_tests', {}).items():
                result = self.run_performance_test(test_config)
                suite_results['tests'].append(result)
            
            # Run security tests
            for test_name, test_config in test_scenarios.get('security_tests', {}).items():
                result = self.run_security_test(test_config)
                suite_results['tests'].append(result)
            
            # Run cultural tests
            for test_name, test_config in test_scenarios.get('cultural_tests', {}).items():
                result = self.run_cultural_test(test_config)
                suite_results['tests'].append(result)
            
            # Calculate summary
            total_tests = len(suite_results['tests'])
            successful_tests = sum(1 for test in suite_results['tests'] if test['success'])
            
            suite_results['summary'] = {
                'total_tests': total_tests,
                'successful_tests': successful_tests,
                'failed_tests': total_tests - successful_tests,
                'success_rate': (successful_tests / total_tests * 100) if total_tests > 0 else 0
            }
            
            suite_results['end_time'] = datetime.now().isoformat()
            
            logger.info(f"Test suite completed. Success rate: {suite_results['summary']['success_rate']:.1f}%")
            
            return suite_results
        
        def save_test_results(self, results: Dict[str, Any]):
            """Save test results"""
            try:
                filename = f"/tmp/alert_test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                with open(filename, 'w') as f:
                    json.dump(results, f, indent=2)
                logger.info(f"Test results saved to {filename}")
            except Exception as e:
                logger.error(f"Failed to save test results: {e}")
        
        def run_daily_tests(self):
            """Run daily alert tests"""
            logger.info("Running daily alert tests")
            results = self.run_test_suite()
            self.save_test_results(results)
            
            # Send summary alert if tests fail
            if results['summary']['success_rate'] < 80:
                self.send_test_alert(
                    'AlertTestingFailure',
                    'warning',
                    {'success_rate': str(results['summary']['success_rate'])}
                )
        
        def run(self):
            """Main testing loop"""
            logger.info("Starting Alert Testing Framework")
            
            # Schedule daily tests
            schedule.every().day.at("02:00").do(self.run_daily_tests)
            
            # Run initial test
            self.run_daily_tests()
            
            while True:
                try:
                    schedule.run_pending()
                    time.sleep(300)  # Check every 5 minutes
                except KeyboardInterrupt:
                    logger.info("Shutting down alert testing framework")
                    break
                except Exception as e:
                    logger.error(f"Error in main loop: {e}")
                    time.sleep(60)
    
    if __name__ == "__main__":
        tester = AlertTester()
        tester.run()

---
# ServiceAccount for alert testing
apiVersion: v1
kind: ServiceAccount
metadata:
  name: alert-tester
  namespace: monitoring

---
# ClusterRole for alert testing
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: alert-tester
rules:
- apiGroups: [""]
  resources: ["pods", "services"]
  verbs: ["get", "list", "create", "delete"]
- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["get", "list", "create", "delete"]
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list"]

---
# ClusterRoleBinding for alert testing
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: alert-tester
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: alert-tester
subjects:
- kind: ServiceAccount
  name: alert-tester
  namespace: monitoring

---
# Service for alert testing framework
apiVersion: v1
kind: Service
metadata:
  name: alert-testing-service
  namespace: monitoring
  labels:
    app: alert-testing-framework
spec:
  selector:
    app: alert-testing-framework
  ports:
  - name: http
    port: 8080
    targetPort: 8080
  type: ClusterIP