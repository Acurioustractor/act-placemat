# ACT Placemat Enhanced Logging Stack
# Centralized logging with Fluentd, Elasticsearch, and Kibana

---
# ========================================
# ELASTICSEARCH DEPLOYMENT
# ========================================

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: elasticsearch
  namespace: monitoring
  labels:
    app: elasticsearch
spec:
  serviceName: elasticsearch
  replicas: 3
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      containers:
      - name: elasticsearch
        image: docker.elastic.co/elasticsearch/elasticsearch:8.8.0
        ports:
        - containerPort: 9200
          name: http
        - containerPort: 9300
          name: transport
        env:
        - name: cluster.name
          value: "act-placemat-logs"
        - name: node.name
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: discovery.seed_hosts
          value: "elasticsearch-0.elasticsearch,elasticsearch-1.elasticsearch,elasticsearch-2.elasticsearch"
        - name: cluster.initial_master_nodes
          value: "elasticsearch-0,elasticsearch-1,elasticsearch-2"
        - name: ES_JAVA_OPTS
          value: "-Xms2g -Xmx2g"
        - name: xpack.security.enabled
          value: "false"
        - name: xpack.monitoring.collection.enabled
          value: "true"
        volumeMounts:
        - name: elasticsearch-storage
          mountPath: /usr/share/elasticsearch/data
        resources:
          requests:
            memory: "3Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        readinessProbe:
          httpGet:
            path: /_cluster/health
            port: 9200
          initialDelaySeconds: 60
          periodSeconds: 10
        livenessProbe:
          httpGet:
            path: /_cluster/health?local=true
            port: 9200
          initialDelaySeconds: 120
          periodSeconds: 30
  volumeClaimTemplates:
  - metadata:
      name: elasticsearch-storage
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: gp3
      resources:
        requests:
          storage: 100Gi

---
# ========================================
# KIBANA DEPLOYMENT
# ========================================

apiVersion: apps/v1
kind: Deployment
metadata:
  name: kibana
  namespace: monitoring
  labels:
    app: kibana
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kibana
  template:
    metadata:
      labels:
        app: kibana
    spec:
      containers:
      - name: kibana
        image: docker.elastic.co/kibana/kibana:8.8.0
        ports:
        - containerPort: 5601
        env:
        - name: ELASTICSEARCH_HOSTS
          value: "http://elasticsearch-service:9200"
        - name: SERVER_NAME
          value: "kibana"
        - name: SERVER_HOST
          value: "0.0.0.0"
        - name: XPACK_MONITORING_ENABLED
          value: "true"
        - name: XPACK_MONITORING_COLLECTION_ENABLED
          value: "true"
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        readinessProbe:
          httpGet:
            path: /api/status
            port: 5601
          initialDelaySeconds: 60
          periodSeconds: 10
        livenessProbe:
          httpGet:
            path: /api/status
            port: 5601
          initialDelaySeconds: 120
          periodSeconds: 30

---
# ========================================
# FLUENTD DAEMONSET
# ========================================

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: monitoring
  labels:
    app: fluentd
spec:
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
    spec:
      serviceAccountName: fluentd
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      - key: node-role.kubernetes.io/control-plane
        effect: NoSchedule
      containers:
      - name: fluentd
        image: fluent/fluentd-kubernetes-daemonset:v1.16-debian-elasticsearch7-1
        env:
        - name: FLUENT_ELASTICSEARCH_HOST
          value: "elasticsearch-service"
        - name: FLUENT_ELASTICSEARCH_PORT
          value: "9200"
        - name: FLUENT_ELASTICSEARCH_SCHEME
          value: "http"
        - name: FLUENTD_SYSTEMD_CONF
          value: "disable"
        - name: FLUENT_ELASTICSEARCH_SSL_VERIFY
          value: "false"
        - name: FLUENT_ELASTICSEARCH_SSL_VERSION
          value: "TLSv1_2"
        - name: FLUENT_ELASTICSEARCH_BUFFER_CHUNK_LIMIT_SIZE
          value: "2M"
        - name: FLUENT_ELASTICSEARCH_BUFFER_QUEUE_LIMIT_LENGTH
          value: "8"
        - name: FLUENT_ELASTICSEARCH_BUFFER_FLUSH_INTERVAL
          value: "5s"
        - name: FLUENT_ELASTICSEARCH_BUFFER_RETRY_LIMIT
          value: "2"
        - name: FLUENT_ELASTICSEARCH_BUFFER_RETRY_WAIT
          value: "1.0"
        - name: FLUENT_ELASTICSEARCH_BUFFER_MAX_RETRY_WAIT
          value: "30"
        - name: FLUENT_ELASTICSEARCH_RELOAD_CONNECTIONS
          value: "false"
        - name: FLUENT_ELASTICSEARCH_RELOAD_ON_FAILURE
          value: "true"
        - name: FLUENT_ELASTICSEARCH_LOG_ES_400_REASON
          value: "false"
        - name: FLUENT_ELASTICSEARCH_LOGSTASH_PREFIX
          value: "act-placemat"
        - name: FLUENT_ELASTICSEARCH_LOGSTASH_FORMAT
          value: "true"
        - name: FLUENT_ELASTICSEARCH_LOGSTASH_DATEFORMAT
          value: "%Y.%m.%d"
        resources:
          requests:
            memory: "200Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: fluentd-config
          mountPath: /fluentd/etc/fluent.conf
          subPath: fluent.conf
        - name: fluentd-config
          mountPath: /fluentd/etc/kubernetes.conf
          subPath: kubernetes.conf
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: fluentd-config
        configMap:
          name: fluentd-config

---
# ========================================
# FLUENTD CONFIGURATION
# ========================================

apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
  namespace: monitoring
data:
  fluent.conf: |
    @include kubernetes.conf
    @include conf.d/*.conf
    
    # Community Platform Specific Parsing
    <filter kubernetes.**>
      @type parser
      key_name log
      reserve_data true
      remove_key_name_field true
      <parse>
        @type multi_format
        <pattern>
          format json
          time_key timestamp
          time_format %Y-%m-%dT%H:%M:%S.%LZ
        </pattern>
        <pattern>
          format regexp
          expression /^(?<timestamp>\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}\.\d{3}Z)\s+(?<level>\w+)\s+(?<message>.*)$/
          time_key timestamp
          time_format %Y-%m-%dT%H:%M:%S.%LZ
        </pattern>
        <pattern>
          format none
        </pattern>
      </parse>
    </filter>

    # Enrich logs with ACT Placemat specific metadata
    <filter kubernetes.**>
      @type record_transformer
      enable_ruby true
      <record>
        cluster_name "act-placemat-production"
        environment "production"
        platform "act-placemat"
        log_processed_at ${Time.now.iso8601}
        kubernetes_namespace ${record["kubernetes"]["namespace_name"]}
        kubernetes_pod ${record["kubernetes"]["pod_name"]}
        kubernetes_container ${record["kubernetes"]["container_name"]}
      </record>
    </filter>

    # Community-specific log filtering
    <filter kubernetes.**>
      @type grep
      <regexp>
        key kubernetes_namespace
        pattern ^(act-production|act-communities|monitoring)$
      </regexp>
    </filter>

    # Error log detection and tagging
    <filter kubernetes.**>
      @type record_transformer
      enable_ruby true
      <record>
        log_level ${record["level"] || (record["log"] && record["log"].match(/ERROR|FATAL/i) ? "ERROR" : (record["log"] && record["log"].match(/WARN/i) ? "WARN" : "INFO"))}
        is_error ${record["log"] && record["log"].match(/ERROR|FATAL|Exception|Error:/i) ? true : false}
        community_context ${record["log"] && record["log"].match(/community_id[=:]\s*(\w+)/i) ? $1 : nil}
        user_context ${record["log"] && record["log"].match(/user_id[=:]\s*(\w+)/i) ? $1 : nil}
        request_id ${record["log"] && record["log"].match(/request_id[=:]\s*([^\s]+)/i) ? $1 : nil}
      </record>
    </filter>

    # Performance metrics extraction
    <filter kubernetes.**>
      @type record_transformer
      enable_ruby true
      <record>
        response_time_ms ${record["log"] && record["log"].match(/response_time[=:]\s*(\d+)/i) ? $1.to_i : nil}
        database_query_time_ms ${record["log"] && record["log"].match(/db_time[=:]\s*(\d+)/i) ? $1.to_i : nil}
        memory_usage_mb ${record["log"] && record["log"].match(/memory_usage[=:]\s*(\d+)/i) ? $1.to_i : nil}
      </record>
    </filter>

    # Business metrics extraction
    <filter kubernetes.**>
      @type record_transformer
      enable_ruby true
      <record>
        story_action ${record["log"] && record["log"].match(/story_(created|updated|deleted|viewed)/i) ? $1 : nil}
        community_action ${record["log"] && record["log"].match(/community_(joined|left|created)/i) ? $1 : nil}
        user_action ${record["log"] && record["log"].match(/user_(registered|login|logout)/i) ? $1 : nil}
        opportunity_action ${record["log"] && record["log"].match(/opportunity_(viewed|applied|shared)/i) ? $1 : nil}
      </record>
    </filter>

    # Output to Elasticsearch
    <match kubernetes.**>
      @type elasticsearch
      host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
      port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
      scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME'] || 'http'}"
      ssl_verify "#{ENV['FLUENT_ELASTICSEARCH_SSL_VERIFY'] || 'true'}"
      ssl_version "#{ENV['FLUENT_ELASTICSEARCH_SSL_VERSION'] || 'TLSv1_2'}"
      reload_connections "#{ENV['FLUENT_ELASTICSEARCH_RELOAD_CONNECTIONS'] || 'false'}"
      reconnect_on_error "#{ENV['FLUENT_ELASTICSEARCH_RELOAD_ON_FAILURE'] || 'true'}"
      reload_on_failure "#{ENV['FLUENT_ELASTICSEARCH_RELOAD_ON_FAILURE'] || 'true'}"
      log_es_400_reason "#{ENV['FLUENT_ELASTICSEARCH_LOG_ES_400_REASON'] || 'false'}"
      logstash_prefix "#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_PREFIX'] || 'logstash'}"
      logstash_format "#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_FORMAT'] || 'true'}"
      logstash_dateformat "#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_DATEFORMAT'] || '%Y.%m.%d'}"
      index_name act-placemat
      type_name _doc
      include_timestamp true
      <buffer>
        @type file
        path /var/log/fluentd-buffers/kubernetes.system.buffer
        flush_mode interval
        retry_type exponential_backoff
        flush_thread_count 2
        flush_interval "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_FLUSH_INTERVAL'] || '5s'}"
        retry_forever
        retry_max_interval 30
        chunk_limit_size "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_CHUNK_LIMIT_SIZE'] || '2M'}"
        queue_limit_length "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_QUEUE_LIMIT_LENGTH'] || '8'}"
        overflow_action block
      </buffer>
    </match>

  kubernetes.conf: |
    <match fluent.**>
      @type null
    </match>

    <source>
      @type tail
      @id in_tail_container_logs
      path /var/log/containers/*.log
      pos_file /var/log/fluentd-containers.log.pos
      tag kubernetes.*
      read_from_head true
      <parse>
        @type multi_format
        <pattern>
          format json
          time_key time
          time_format %Y-%m-%dT%H:%M:%S.%NZ
        </pattern>
        <pattern>
          format /^(?<time>.+) (?<stream>stdout|stderr) [^ ]* (?<log>.*)$/
          time_format %Y-%m-%dT%H:%M:%S.%N%:z
        </pattern>
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_minion
      path /var/log/salt/minion
      pos_file /var/log/fluentd-salt.log.pos
      tag salt
      <parse>
        @type regexp
        expression /^(?<time>[^ ]* [^ ,]*)[^\[]*\[[^\]]*\]\[(?<severity>[^ \]]*) *\] (?<message>.*)$/
        time_format %Y-%m-%d %H:%M:%S
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_startupscript
      path /var/log/startupscript.log
      pos_file /var/log/fluentd-startupscript.log.pos
      tag startupscript
      <parse>
        @type syslog
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_docker
      path /var/log/docker.log
      pos_file /var/log/fluentd-docker.log.pos
      tag docker
      <parse>
        @type regexp
        expression /^time="(?<time>[^)]*)" level=(?<severity>[^ ]*) msg="(?<message>[^"]*)"( err="(?<error>[^"]*)")?( statusCode=($<status_code>\d+))?/
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_etcd
      path /var/log/etcd.log
      pos_file /var/log/fluentd-etcd.log.pos
      tag etcd
      <parse>
        @type none
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_kubelet
      path /var/log/kubelet.log
      pos_file /var/log/fluentd-kubelet.log.pos
      tag kubelet
      <parse>
        @type kubernetes
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_kube_proxy
      path /var/log/kube-proxy.log
      pos_file /var/log/fluentd-kube-proxy.log.pos
      tag kube-proxy
      <parse>
        @type kubernetes
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_kube_apiserver
      path /var/log/kube-apiserver.log
      pos_file /var/log/fluentd-kube-apiserver.log.pos
      tag kube-apiserver
      <parse>
        @type kubernetes
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_kube_controller_manager
      path /var/log/kube-controller-manager.log
      pos_file /var/log/fluentd-kube-controller-manager.log.pos
      tag kube-controller-manager
      <parse>
        @type kubernetes
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_kube_scheduler
      path /var/log/kube-scheduler.log
      pos_file /var/log/fluentd-kube-scheduler.log.pos
      tag kube-scheduler
      <parse>
        @type kubernetes
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_rescheduler
      path /var/log/rescheduler.log
      pos_file /var/log/fluentd-rescheduler.log.pos
      tag rescheduler
      <parse>
        @type kubernetes
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_glbc
      path /var/log/glbc.log
      pos_file /var/log/fluentd-glbc.log.pos
      tag glbc
      <parse>
        @type kubernetes
      </parse>
    </source>

    <source>
      @type tail
      @id in_tail_cluster_autoscaler
      path /var/log/cluster-autoscaler.log
      pos_file /var/log/fluentd-cluster-autoscaler.log.pos
      tag cluster-autoscaler
      <parse>
        @type kubernetes
      </parse>
    </source>

    <filter kubernetes.**>
      @type kubernetes_metadata
    </filter>

---
# ========================================
# SERVICES
# ========================================

apiVersion: v1
kind: Service
metadata:
  name: elasticsearch-service
  namespace: monitoring
  labels:
    app: elasticsearch
spec:
  selector:
    app: elasticsearch
  ports:
  - name: http
    port: 9200
    targetPort: 9200
  - name: transport
    port: 9300
    targetPort: 9300
  type: ClusterIP

---
apiVersion: v1
kind: Service
metadata:
  name: kibana-service
  namespace: monitoring
  labels:
    app: kibana
spec:
  selector:
    app: kibana
  ports:
  - name: web
    port: 5601
    targetPort: 5601
  type: ClusterIP

---
# ========================================
# SERVICE ACCOUNT AND RBAC FOR FLUENTD
# ========================================

apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluentd
  namespace: monitoring

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: fluentd
rules:
- apiGroups: [""]
  resources:
  - pods
  - namespaces
  verbs: ["get", "list", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: fluentd
roleRef:
  kind: ClusterRole
  name: fluentd
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: fluentd
  namespace: monitoring

---
# ========================================
# LOGSTASH FOR ADDITIONAL PROCESSING
# ========================================

apiVersion: apps/v1
kind: Deployment
metadata:
  name: logstash
  namespace: monitoring
  labels:
    app: logstash
spec:
  replicas: 2
  selector:
    matchLabels:
      app: logstash
  template:
    metadata:
      labels:
        app: logstash
    spec:
      containers:
      - name: logstash
        image: docker.elastic.co/logstash/logstash:8.8.0
        ports:
        - containerPort: 5044
        - containerPort: 9600
        env:
        - name: LS_JAVA_OPTS
          value: "-Xmx2g -Xms2g"
        volumeMounts:
        - name: logstash-config
          mountPath: /usr/share/logstash/pipeline
        - name: logstash-settings
          mountPath: /usr/share/logstash/config/logstash.yml
          subPath: logstash.yml
        resources:
          requests:
            memory: "2Gi"
            cpu: "500m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        readinessProbe:
          httpGet:
            path: /
            port: 9600
          initialDelaySeconds: 60
          periodSeconds: 10
        livenessProbe:
          httpGet:
            path: /
            port: 9600
          initialDelaySeconds: 120
          periodSeconds: 30
      volumes:
      - name: logstash-config
        configMap:
          name: logstash-config
      - name: logstash-settings
        configMap:
          name: logstash-settings

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: logstash-config
  namespace: monitoring
data:
  act-placemat.conf: |
    input {
      beats {
        port => 5044
      }
      elasticsearch {
        hosts => ["elasticsearch-service:9200"]
        index => "act-placemat-*"
        query => '{ "query": { "match": { "is_error": true } } }'
        schedule => "*/5 * * * *"
        type => "error_analysis"
      }
    }

    filter {
      # Parse ACT Placemat specific log formats
      if [kubernetes][container_name] =~ /backend|frontend|insights-engine/ {
        # Extract performance metrics
        grok {
          match => { "message" => "response_time=%{NUMBER:response_time_ms:float}" }
          tag_on_failure => ["_grokparsefailure_response_time"]
        }
        
        # Extract business metrics
        grok {
          match => { "message" => "story_(created|updated|deleted|viewed) story_id=%{WORD:story_id} user_id=%{WORD:user_id}" }
          tag_on_failure => ["_grokparsefailure_story_action"]
        }
        
        # Extract community metrics
        grok {
          match => { "message" => "community_(joined|left|created) community_id=%{WORD:community_id} user_id=%{WORD:user_id}" }
          tag_on_failure => ["_grokparsefailure_community_action"]
        }
        
        # Calculate derived metrics
        if [response_time_ms] {
          if [response_time_ms] > 2000 {
            mutate { add_field => { "slow_response" => true } }
          }
          if [response_time_ms] > 5000 {
            mutate { add_field => { "very_slow_response" => true } }
          }
        }
      }
      
      # Anonymize sensitive data
      mutate {
        gsub => [
          "message", "\b\d{4}\s?\d{4}\s?\d{4}\s?\d{4}\b", "[CREDIT_CARD]",
          "message", "\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b", "[EMAIL]",
          "message", "\b(?:\+61|0)[0-9]{9}\b", "[PHONE]"
        ]
      }
      
      # Add geographic context for Australian regions
      if [client_ip] {
        geoip {
          source => "client_ip"
          target => "geoip"
          add_field => [ "[geoip][coordinates]", "%{[geoip][longitude]}" ]
          add_field => [ "[geoip][coordinates]", "%{[geoip][latitude]}"  ]
        }
        mutate {
          convert => [ "[geoip][coordinates]", "float"]
        }
      }
      
      # Tag Australian-specific content
      if [geoip][country_code2] == "AU" {
        mutate { add_field => { "australian_user" => true } }
        
        # Add state/territory information
        if [geoip][region_name] {
          mutate { add_field => { "australian_state" => "%{[geoip][region_name]}" } }
        }
      }
    }

    output {
      elasticsearch {
        hosts => ["elasticsearch-service:9200"]
        index => "act-placemat-processed-%{+YYYY.MM.dd}"
      }
      
      # Send alerts for critical errors
      if [log_level] == "ERROR" or [is_error] == true {
        http {
          url => "http://alertmanager-service:9093/api/v1/alerts"
          http_method => "post"
          format => "json"
          mapping => {
            "alerts" => [{
              "labels" => {
                "alertname" => "ApplicationError"
                "severity" => "warning"
                "service" => "%{[kubernetes][container_name]}"
                "namespace" => "%{[kubernetes][namespace_name]}"
                "pod" => "%{[kubernetes][pod_name]}"
              }
              "annotations" => {
                "summary" => "Application error detected"
                "description" => "%{message}"
              }
            }]
          }
        }
      }
      
      # Output performance metrics to separate index
      if [response_time_ms] or [database_query_time_ms] or [memory_usage_mb] {
        elasticsearch {
          hosts => ["elasticsearch-service:9200"]
          index => "act-placemat-metrics-%{+YYYY.MM.dd}"
        }
      }
      
      # Debug output (remove in production)
      stdout { codec => rubydebug }
    }

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: logstash-settings
  namespace: monitoring
data:
  logstash.yml: |
    http.host: "0.0.0.0"
    path.config: /usr/share/logstash/pipeline
    xpack.monitoring.enabled: true
    xpack.monitoring.elasticsearch.hosts: ["http://elasticsearch-service:9200"]

---
apiVersion: v1
kind: Service
metadata:
  name: logstash-service
  namespace: monitoring
  labels:
    app: logstash
spec:
  selector:
    app: logstash
  ports:
  - name: beats
    port: 5044
    targetPort: 5044
  - name: http
    port: 9600
    targetPort: 9600
  type: ClusterIP