#!/bin/bash
# Check Ollama download progress

echo "üîç Checking Ollama status..."
echo ""

# Check if Ollama is running
if ! pgrep -x "ollama" > /dev/null; then
    echo "‚ùå Ollama service not running"
    echo "   Start with: brew services start ollama"
    exit 1
fi

echo "‚úÖ Ollama service is running"
echo ""

# Check download progress
echo "üì• Download status:"
tail -1 /tmp/ollama-download.log 2>/dev/null || echo "   No active download"
echo ""

# List available models
echo "üì¶ Available models:"
ollama list
echo ""

# If llama3.1:8b is available, test it
if ollama list | grep -q "llama3.1:8b"; then
    echo "üéâ llama3.1:8b is ready!"
    echo ""
    echo "‚úÖ Next steps:"
    echo "   1. Add to your .env: OLLAMA_URL=http://localhost:11434"
    echo "   2. Restart your backend"
    echo "   3. Test Deep mode with High Privacy in the AI Agent"
else
    echo "‚è≥ llama3.1:8b still downloading..."
    echo ""
    echo "   Check progress with: tail -f /tmp/ollama-download.log"
    echo "   Or run this script again: ./check-ollama-progress.sh"
fi
